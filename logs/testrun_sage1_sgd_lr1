nohup: ignoring input
wandb: Currently logged in as: ajh. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.4
wandb: Run data is saved locally in /home/ajh/chgcnn/wandb/run-20230301_010339-2jalbxod
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-valley-68
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ajh/chgcnn
wandb: üöÄ View run at https://wandb.ai/ajh/chgcnn/runs/2jalbxod
Importing data from und_hetero_relgraph_list.pkl
Initializing model...
Using MSE training loss and L1 for accuracy
/home/ajh/anaconda3/envs/chgcnn/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ajh/anaconda3/envs/chgcnn/lib/python3.10/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.l1_loss(input, target, reduction=self.reduction)
Epoch: [1][    0/16745]	Batch 0.0229 (0.0229)	Data 0.0029 (0.0029)	Loss 1.3686 (1.3686)	Accu 1.1699 (1.1699)
Epoch: [1][   10/16745]	Batch 0.0220 (0.0212)	Data 0.0022 (0.0023)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][   20/16745]	Batch 0.0222 (0.0215)	Data 0.0029 (0.0024)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][   30/16745]	Batch 0.0206 (0.0214)	Data 0.0023 (0.0024)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][   40/16745]	Batch 0.0220 (0.0212)	Data 0.0020 (0.0024)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][   50/16745]	Batch 0.0205 (0.0212)	Data 0.0029 (0.0024)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][   60/16745]	Batch 0.0211 (0.0212)	Data 0.0021 (0.0024)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][   70/16745]	Batch 0.0223 (0.0213)	Data 0.0024 (0.0024)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][   80/16745]	Batch 0.0220 (0.0213)	Data 0.0020 (0.0024)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][   90/16745]	Batch 0.0209 (0.0212)	Data 0.0029 (0.0024)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  100/16745]	Batch 0.0209 (0.0212)	Data 0.0024 (0.0024)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  110/16745]	Batch 0.0206 (0.0212)	Data 0.0022 (0.0024)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  120/16745]	Batch 0.0207 (0.0212)	Data 0.0022 (0.0024)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  130/16745]	Batch 0.0219 (0.0212)	Data 0.0020 (0.0024)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  140/16745]	Batch 0.0205 (0.0213)	Data 0.0021 (0.0024)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  150/16745]	Batch 0.0214 (0.0213)	Data 0.0021 (0.0024)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  160/16745]	Batch 0.0221 (0.0213)	Data 0.0022 (0.0024)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  170/16745]	Batch 0.0209 (0.0213)	Data 0.0024 (0.0024)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  180/16745]	Batch 0.0222 (0.0215)	Data 0.0023 (0.0024)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  190/16745]	Batch 0.0208 (0.0215)	Data 0.0024 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  200/16745]	Batch 0.0225 (0.0215)	Data 0.0022 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  210/16745]	Batch 0.0209 (0.0215)	Data 0.0023 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  220/16745]	Batch 0.0207 (0.0215)	Data 0.0021 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  230/16745]	Batch 0.0199 (0.0215)	Data 0.0029 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  240/16745]	Batch 0.0223 (0.0215)	Data 0.0024 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  250/16745]	Batch 0.0204 (0.0215)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  260/16745]	Batch 0.0208 (0.0215)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  270/16745]	Batch 0.0346 (0.0216)	Data 0.0061 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  280/16745]	Batch 0.0207 (0.0216)	Data 0.0024 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  290/16745]	Batch 0.0224 (0.0217)	Data 0.0032 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  300/16745]	Batch 0.0228 (0.0217)	Data 0.0028 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  310/16745]	Batch 0.0223 (0.0217)	Data 0.0030 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  320/16745]	Batch 0.0213 (0.0217)	Data 0.0026 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  330/16745]	Batch 0.0208 (0.0216)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  340/16745]	Batch 0.0221 (0.0216)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  350/16745]	Batch 0.0222 (0.0216)	Data 0.0028 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  360/16745]	Batch 0.0220 (0.0217)	Data 0.0023 (0.0026)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  370/16745]	Batch 0.0226 (0.0217)	Data 0.0027 (0.0026)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  380/16745]	Batch 0.0196 (0.0216)	Data 0.0028 (0.0026)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  390/16745]	Batch 0.0205 (0.0216)	Data 0.0021 (0.0026)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  400/16745]	Batch 0.0221 (0.0216)	Data 0.0024 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  410/16745]	Batch 0.0209 (0.0216)	Data 0.0029 (0.0026)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  420/16745]	Batch 0.0213 (0.0216)	Data 0.0024 (0.0026)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  430/16745]	Batch 0.0207 (0.0216)	Data 0.0021 (0.0026)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  440/16745]	Batch 0.0216 (0.0216)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  450/16745]	Batch 0.0210 (0.0216)	Data 0.0022 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  460/16745]	Batch 0.0200 (0.0216)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  470/16745]	Batch 0.0204 (0.0216)	Data 0.0021 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  480/16745]	Batch 0.0206 (0.0216)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  490/16745]	Batch 0.0222 (0.0216)	Data 0.0033 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  500/16745]	Batch 0.0219 (0.0216)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  510/16745]	Batch 0.0205 (0.0216)	Data 0.0022 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  520/16745]	Batch 0.0219 (0.0216)	Data 0.0026 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  530/16745]	Batch 0.0204 (0.0215)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  540/16745]	Batch 0.0209 (0.0215)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  550/16745]	Batch 0.0204 (0.0215)	Data 0.0021 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  560/16745]	Batch 0.0199 (0.0215)	Data 0.0021 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  570/16745]	Batch 0.0204 (0.0215)	Data 0.0023 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  580/16745]	Batch 0.0196 (0.0215)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  590/16745]	Batch 0.0213 (0.0215)	Data 0.0031 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  600/16745]	Batch 0.0222 (0.0215)	Data 0.0022 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  610/16745]	Batch 0.0205 (0.0215)	Data 0.0021 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  620/16745]	Batch 0.0226 (0.0215)	Data 0.0026 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  630/16745]	Batch 0.0222 (0.0215)	Data 0.0032 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  640/16745]	Batch 0.0218 (0.0216)	Data 0.0026 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  650/16745]	Batch 0.0223 (0.0216)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  660/16745]	Batch 0.0225 (0.0216)	Data 0.0025 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  670/16745]	Batch 0.0208 (0.0216)	Data 0.0022 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  680/16745]	Batch 0.0226 (0.0216)	Data 0.0024 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  690/16745]	Batch 0.0220 (0.0216)	Data 0.0027 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  700/16745]	Batch 0.0208 (0.0216)	Data 0.0025 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  710/16745]	Batch 0.0221 (0.0216)	Data 0.0028 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  720/16745]	Batch 0.0207 (0.0216)	Data 0.0023 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  730/16745]	Batch 0.0206 (0.0216)	Data 0.0022 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  740/16745]	Batch 0.0235 (0.0216)	Data 0.0024 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  750/16745]	Batch 0.0211 (0.0216)	Data 0.0022 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  760/16745]	Batch 0.0206 (0.0216)	Data 0.0023 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  770/16745]	Batch 0.0227 (0.0216)	Data 0.0029 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  780/16745]	Batch 0.0217 (0.0216)	Data 0.0030 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  790/16745]	Batch 0.0210 (0.0216)	Data 0.0025 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  800/16745]	Batch 0.0205 (0.0216)	Data 0.0021 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  810/16745]	Batch 0.0206 (0.0216)	Data 0.0021 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  820/16745]	Batch 0.0207 (0.0216)	Data 0.0021 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  830/16745]	Batch 0.0213 (0.0216)	Data 0.0021 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  840/16745]	Batch 0.0208 (0.0216)	Data 0.0021 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  850/16745]	Batch 0.0207 (0.0216)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  860/16745]	Batch 0.0217 (0.0216)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  870/16745]	Batch 0.0224 (0.0216)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  880/16745]	Batch 0.0209 (0.0216)	Data 0.0027 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  890/16745]	Batch 0.0230 (0.0216)	Data 0.0029 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  900/16745]	Batch 0.0209 (0.0216)	Data 0.0026 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  910/16745]	Batch 0.0213 (0.0216)	Data 0.0023 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  920/16745]	Batch 0.0206 (0.0216)	Data 0.0029 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  930/16745]	Batch 0.0219 (0.0215)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  940/16745]	Batch 0.0220 (0.0216)	Data 0.0025 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  950/16745]	Batch 0.0221 (0.0216)	Data 0.0022 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  960/16745]	Batch 0.0225 (0.0216)	Data 0.0038 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  970/16745]	Batch 0.0220 (0.0216)	Data 0.0021 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  980/16745]	Batch 0.0215 (0.0216)	Data 0.0025 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][  990/16745]	Batch 0.0216 (0.0216)	Data 0.0027 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1000/16745]	Batch 0.0213 (0.0216)	Data 0.0024 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1010/16745]	Batch 0.0202 (0.0215)	Data 0.0021 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1020/16745]	Batch 0.0206 (0.0215)	Data 0.0022 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1030/16745]	Batch 0.0580 (0.0216)	Data 0.0024 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1040/16745]	Batch 0.0208 (0.0216)	Data 0.0023 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1050/16745]	Batch 0.0212 (0.0216)	Data 0.0022 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1060/16745]	Batch 0.0210 (0.0216)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1070/16745]	Batch 0.0207 (0.0216)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1080/16745]	Batch 0.0208 (0.0216)	Data 0.0021 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1090/16745]	Batch 0.0210 (0.0216)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1100/16745]	Batch 0.0206 (0.0216)	Data 0.0021 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1110/16745]	Batch 0.0208 (0.0216)	Data 0.0024 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1120/16745]	Batch 0.0205 (0.0216)	Data 0.0021 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1130/16745]	Batch 0.0205 (0.0216)	Data 0.0022 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1140/16745]	Batch 0.0207 (0.0216)	Data 0.0022 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1150/16745]	Batch 0.0218 (0.0215)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1160/16745]	Batch 0.0209 (0.0216)	Data 0.0033 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1170/16745]	Batch 0.0225 (0.0215)	Data 0.0028 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1180/16745]	Batch 0.0203 (0.0215)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1190/16745]	Batch 0.0225 (0.0215)	Data 0.0022 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1200/16745]	Batch 0.0215 (0.0216)	Data 0.0032 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1210/16745]	Batch 0.0201 (0.0216)	Data 0.0022 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1220/16745]	Batch 0.0200 (0.0215)	Data 0.0025 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1230/16745]	Batch 0.0217 (0.0215)	Data 0.0034 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1240/16745]	Batch 0.0203 (0.0215)	Data 0.0021 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1250/16745]	Batch 0.0206 (0.0215)	Data 0.0032 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1260/16745]	Batch 0.0206 (0.0215)	Data 0.0024 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1270/16745]	Batch 0.0215 (0.0215)	Data 0.0023 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1280/16745]	Batch 0.0224 (0.0215)	Data 0.0030 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1290/16745]	Batch 0.0221 (0.0215)	Data 0.0022 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1300/16745]	Batch 0.0226 (0.0215)	Data 0.0025 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1310/16745]	Batch 0.0229 (0.0215)	Data 0.0029 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1320/16745]	Batch 0.0210 (0.0215)	Data 0.0021 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1330/16745]	Batch 0.0214 (0.0215)	Data 0.0034 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1340/16745]	Batch 0.0206 (0.0215)	Data 0.0023 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1350/16745]	Batch 0.0203 (0.0215)	Data 0.0021 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1360/16745]	Batch 0.0231 (0.0215)	Data 0.0029 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1370/16745]	Batch 0.0217 (0.0215)	Data 0.0055 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1380/16745]	Batch 0.0223 (0.0216)	Data 0.0021 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1390/16745]	Batch 0.0222 (0.0216)	Data 0.0028 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1400/16745]	Batch 0.0248 (0.0216)	Data 0.0032 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1410/16745]	Batch 0.0221 (0.0216)	Data 0.0021 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1420/16745]	Batch 0.0213 (0.0216)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1430/16745]	Batch 0.0198 (0.0215)	Data 0.0033 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1440/16745]	Batch 0.0203 (0.0215)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1450/16745]	Batch 0.0221 (0.0215)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1460/16745]	Batch 0.0208 (0.0215)	Data 0.0023 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1470/16745]	Batch 0.0203 (0.0215)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1480/16745]	Batch 0.0210 (0.0215)	Data 0.0023 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1490/16745]	Batch 0.0210 (0.0215)	Data 0.0023 (0.0025)	Loss nan (nan)	Accu nan (nan)
Epoch: [1][ 1500/16745]	Batch 0.0202 (0.0215)	Data 0.0020 (0.0025)	Loss nan (nan)	Accu nan (nan)
Exception in thread MsgRouterThr:
Traceback (most recent call last):
  File "/home/ajh/anaconda3/envs/chgcnn/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/home/ajh/anaconda3/envs/chgcnn/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ajh/anaconda3/envs/chgcnn/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 70, in message_loop
    msg = self._read_message()
  File "/home/ajh/anaconda3/envs/chgcnn/lib/python3.10/site-packages/wandb/sdk/interface/router_queue.py", line 36, in _read_message
    msg = self._response_queue.get(timeout=1)
  File "/home/ajh/anaconda3/envs/chgcnn/lib/python3.10/multiprocessing/queues.py", line 117, in get
    res = self._recv_bytes()
  File "/home/ajh/anaconda3/envs/chgcnn/lib/python3.10/multiprocessing/connection.py", line 217, in recv_bytes
    self._check_closed()
  File "/home/ajh/anaconda3/envs/chgcnn/lib/python3.10/multiprocessing/connection.py", line 141, in _check_closed
    raise OSError("handle is closed")
OSError: handle is closed
